{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-26 11:51:12.831863: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-09-26 11:51:13.646738: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-26 11:51:13.646828: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-26 11:51:13.646838: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    FLAX_MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    FlaxAutoModelForCausalLM,\n",
    "    HfArgumentParser,\n",
    "    is_tensorboard_available,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    '/home/pphuc/code/EasyLM/model_pretrained/llama/converted',\n",
    "    # trust_remote_code=True,\n",
    "    # dtype=getattr(jnp, 'bf16'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(36001, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=36001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import mlxu\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "from flax.traverse_util import flatten_dict\n",
    "import torch\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "from EasyLM.checkpoint import StreamingCheckpointer\n",
    "from EasyLM.jax_utils import float_tensor_to_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_keywords(string, positives, negatives):\n",
    "    for positive in positives:\n",
    "        if positive not in string:\n",
    "            return False\n",
    "    for negative in negatives:\n",
    "        if negative in string:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_convert_checkpoint(path):\n",
    "    _, flax_params = StreamingCheckpointer.load_trainstate_checkpoint(path)\n",
    "    flax_params = flatten_dict(flax_params['params'], sep='.')\n",
    "    torch_params = {}\n",
    "    for key, tensor in flax_params.items():\n",
    "        if match_keywords(key, [\"kernel\"], [\"norm\", 'ln_f']):\n",
    "            tensor = tensor.T\n",
    "        torch_params[key] = torch.tensor(\n",
    "            float_tensor_to_dtype(tensor, 'fp32'), dtype=torch.float16\n",
    "        )\n",
    "    return torch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'params::/home/pphuc/code/EasyLM/model_pretrained/llama/output_dir/0c3be9e05d4643ecb54b4b5cebb6b3bd/streaming_train_state_500'\n",
    "model = load_and_convert_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_STANDARD_CONFIGS = {\n",
    "    '1b': {\n",
    "        'dim': 2048,\n",
    "        'intermediate_size': 5504,\n",
    "        'n_layers': 22,\n",
    "        'n_heads': 16,\n",
    "        'norm_eps': 1e-6,\n",
    "    },\n",
    "}\n",
    "\n",
    "params = LLAMA_STANDARD_CONFIGS['1b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
